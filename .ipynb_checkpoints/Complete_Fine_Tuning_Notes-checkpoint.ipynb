{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0298d1-46d6-4666-9420-ab785ed3989d",
   "metadata": {},
   "source": [
    "# ✅ **Fine‑Tuning**\n",
    "\n",
    "Fine‑tuning is the process of taking a pre‑trained language model and training it further on your own curated data so it learns specific skills, style, or formats that match your needs. It changes the model’s *default behavior* to follow your preferences without long prompts.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "- **Task Specialization:** Boost accuracy for a specific task (e.g., classification, extraction).\n",
    "- **Domain Adaptation:** Make the model fluent in your jargon and context.\n",
    "- **Style & Format Control:** Ensure consistent tone, voice, and output structure.\n",
    "- **Policy Adherence:** Encode organizational rules and compliance.\n",
    "- **Efficiency:** Reduce prompt length, cost, and latency while improving reliability.\n",
    "\n",
    "---\n",
    "\n",
    "# Types of Fine‑Tuning\n",
    "\n",
    "## 1. Full Fine‑Tuning\n",
    "\n",
    "**Definition:**  \n",
    "Update all parameters of the pre‑trained model on your dataset.\n",
    "\n",
    "**Pros:**  \n",
    "Best performance and flexibility; can deeply adapt to new tasks/domains.\n",
    "\n",
    "**Cons:**  \n",
    "Requires large compute/resources; risk of overfitting or catastrophic forgetting.\n",
    "\n",
    "**Use Case:**  \n",
    "When you have a large, high‑quality dataset and need maximum adaptation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. PEFT (Parameter‑Efficient Fine‑Tuning)\n",
    "\n",
    "**Definition:**  \n",
    "Freeze most of the model’s parameters and train only a small subset or added modules.\n",
    "\n",
    "**Purpose:**  \n",
    "Reduce compute cost, memory usage, and training time while keeping strong performance.\n",
    "\n",
    "**Techniques:**\n",
    "\n",
    "- **LoRA (Low‑Rank Adaptation):** Inject small trainable rank‑decomposition matrices into layers.\n",
    "- **QLoRA:** LoRA on quantized (lower‑precision) base model to save memory.\n",
    "- **Adapters:** Small extra layers between existing ones, trained while the base model stays frozen.\n",
    "- **Prefix Tuning:** Train extra tokens (prefixes) prepended to inputs in each layer, guiding output without changing main weights.\n",
    "\n",
    "**Use Case:**  \n",
    "When resources are limited, or you need multiple task‑specific adapters for the same base model.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Instruction Fine‑Tuning\n",
    "\n",
    "**Definition:**  \n",
    "Supervised fine‑tuning using datasets of instructions and ideal responses, teaching the model to follow natural language instructions reliably.\n",
    "\n",
    "**Purpose:**  \n",
    "Improve general usability, helpfulness, and compliance by training on diverse instruction‑response pairs.\n",
    "\n",
    "**Use Case:**  \n",
    "Creating general instruction‑following models or adapting to specific instruction styles.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **Prompt Engineering vs. Fine‑Tuning**\n",
    "\n",
    "# Prompt Engineering\n",
    "\n",
    "- **Definition:**  \n",
    "  Controlling and steering a model’s behavior at runtime by crafting instructions and optionally providing examples.\n",
    "\n",
    "- **Types:**\n",
    "  - **Zero‑Shot:** Only instructions, no examples.\n",
    "  - **One‑Shot:** One example to show the format or style.\n",
    "  - **Few‑Shot:** Multiple examples (usually 2–10) to guide output.\n",
    "\n",
    "- **Pros:**  \n",
    "  No training required, quick to adjust, works well for many general tasks.\n",
    "\n",
    "- **Cons:**  \n",
    "  Prompts can become long, outputs may be inconsistent, harder to enforce strict formats.\n",
    "\n",
    "- **Best For:**  \n",
    "  Rapid prototyping, evolving requirements, or tasks where occasional variation is acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "# Fine‑Tuning\n",
    "\n",
    "- **Definition:**  \n",
    "  Further training a pre‑trained model on curated examples so it learns your desired behavior, style, or format by default.\n",
    "\n",
    "- **Pros:**  \n",
    "  Consistent and reliable outputs, shorter prompts, better domain and style adherence.\n",
    "\n",
    "- **Cons:**  \n",
    "  Requires dataset preparation, training resources, and time.\n",
    "\n",
    "- **Best For:**  \n",
    "  When prompting alone can’t achieve the accuracy, style, or reliability you need.\n",
    "\n",
    "---\n",
    "\n",
    "# Key Difference\n",
    "\n",
    "Prompting is *temporary steering* (few‑shot examples, no extra training),  \n",
    "while fine‑tuning is *permanent adaptation* (needed when prompting isn’t enough).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbdcef0-272f-46c2-a1df-d86cb468ce79",
   "metadata": {},
   "source": [
    "# ✅ **Quantization & Model Optimization**\n",
    "\n",
    "## Goal\n",
    "Make fine‑tuned models smaller, faster, and more efficient without significantly hurting performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept\n",
    "\n",
    "Quantization reduces the numerical precision of model weights and activations so they require less memory and compute:\n",
    "\n",
    "- **FP32 → FP16 → INT8 → INT4**\n",
    "\n",
    "  - **FP32 (32‑bit floating point):** High precision, large size, slower inference.\n",
    "  - **FP16 (16‑bit floating point):** Half the size, faster compute, minimal accuracy loss.\n",
    "  - **INT8 (8‑bit integer):** Much smaller, faster, slightly more accuracy loss possible.\n",
    "  - **INT4 (4‑bit integer):** Extremely compact, enables very large models on small hardware, but higher risk of accuracy drop.\n",
    "\n",
    "**Example:**  \n",
    "A 13B parameter model in FP32 (50GB) -> INT4 (12GB) can now run on a single consumer GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Quantization\n",
    "\n",
    "### 1. Post‑Training Quantization (PTQ)\n",
    "\n",
    "- **Definition:** Apply quantization *after* the model is fully trained.\n",
    "- **Pros:** Fast, easy, no retraining.\n",
    "- **Cons:** Might cause higher accuracy loss for sensitive tasks.\n",
    "- **Example:** Train model in FP32, then convert weights to INT8 using a tool like `BitsAndBytes`.\n",
    "\n",
    "### 2. Quantization‑Aware Training (QAT)\n",
    "\n",
    "- **Definition:** Simulate quantization *during* training so the model learns to work within reduced precision.\n",
    "- **Pros:** Higher accuracy than PTQ because the model adapts during training.\n",
    "- **Cons:** More complex, requires retraining.\n",
    "- **Example:** During fine‑tuning, activations and weights are simulated as INT8, so the model learns to tolerate it.\n",
    "\n",
    "### 3. Mixed Precision Training\n",
    "\n",
    "- **Definition:** Use different precisions for different operations.\n",
    "- **Pros:** Faster training and reduced memory without major accuracy drop.\n",
    "- **Cons:** Slight complexity in setup.\n",
    "- **Example:** Use FP16 for matrix multiplications (speed), FP32 for loss calculations (precision).\n",
    "\n",
    "---\n",
    "\n",
    "## Popular Libraries\n",
    "\n",
    "- **BitsAndBytes:**\n",
    "  - Allows 4‑bit quantization with minimal accuracy loss.\n",
    "  - QLoRA: fine‑tune a 4‑bit base model using LoRA adapters.\n",
    "  - **Benefit:** Train large models on smaller GPUs.\n",
    "\n",
    "- **GPTQ(General Purpose Quantization for Transformers):**\n",
    "  - Post‑training quantization method optimized for inference.\n",
    "  - Focused on minimizing accuracy loss with per‑channel quantization.\n",
    "\n",
    "- **AWQ (Activation‑aware Weight Quantization):**\n",
    "  - Optimizes quantization by analyzing activation ranges.\n",
    "  - Often yields better performance than GPTQ for some models.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Method          | When Applied              | Precision     | Pros                     | Cons                       |\n",
    "| --------------- | ------------------------- | ------------- | ------------------------ | -------------------------- |\n",
    "| PTQ             | After training            | INT8/INT4     | Fast, simple             | Accuracy loss risk         |\n",
    "| QAT             | During training           | INT8/INT4     | Better accuracy          | More compute/training time |\n",
    "| Mixed Precision | During training/inference | FP16/FP32 mix | Speed + accuracy balance | Slight complexity          |\n",
    "\n",
    "\n",
    "- Quantization and model optimization make fine‑tuned models more accessible on smaller hardware, enabling faster, cheaper inference and training.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **Adapter in Fine-Tuning**\n",
    "\n",
    "An **adapter** is a small set of **extra trainable parameters** added inside a pre-trained model to learn new skills or styles **without changing the original model weights**.\n",
    "\n",
    "Think of it like a **plug‑in** for a model:\n",
    "\n",
    "- **Base model** stays frozen (unchanged).\n",
    "- **Adapter** learns the new domain/task.\n",
    "- At inference, inputs pass through both — base + adapter — to produce adapted outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use Adapters?\n",
    "\n",
    "1. **Efficient Training** — Only a few million parameters are trained, not billions.\n",
    "2. **Multi-Task Flexibility** — Swap adapters for different tasks/domains.\n",
    "3. **Low Storage** — Adapters are MBs in size vs GBs for full models.\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works ?\n",
    "\n",
    "1. Insert **small trainable weights** inside existing layers (often after attention or feed-forward blocks).\n",
    "2. **Freeze** all original parameters.\n",
    "3. Train **only** the adapter on your dataset.\n",
    "4. Save just the adapter weights.\n",
    "5. At inference, load base + adapter for the adapted behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "A base model is a **general writer**. To make it write **legal contracts**:\n",
    "\n",
    "- Add small adapter weights that learn legal terms, tone, and format.\n",
    "- For medical reports, train another adapter and swap it in.\n",
    "\n",
    "---\n",
    "\n",
    "## Popular Adapter Methods\n",
    "\n",
    "- **LoRA** — Low‑Rank Adaptation; small matrices added to attention/FFN layers.\n",
    "- **Prefix Tuning** — Trainable “virtual tokens” prepended in model layers.\n",
    "- **Standard Adapters** — Tiny feed‑forward blocks between Transformer layers.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
